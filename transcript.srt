1
00:00:00,000 --> 00:00:04,219
Every programmer has run into at least one sorting algorithm at one point in their career.

2
00:00:04,400 --> 00:00:09,720
Today, I'm going to easily explain 10 of the most popular sorting algorithms,  as well as the pros and cons of each.

3
00:00:10,359 --> 00:00:10,960
Let's go.

4
00:00:14,280 --> 00:00:18,879
BubbleSaur is one of the most popular sorting algorithms,  probably because it's the easiest.

5
00:00:19,420 --> 00:00:22,199
BubbleSaur has an index that goes to the entire list.

6
00:00:22,399 --> 00:00:26,460
If the number is on is larger than the next item, it switches them and then moves forward.

7
00:00:26,460 --> 00:00:31,100
It then repeats this until every single item in the list has properly been solved.

8
00:00:31,420 --> 00:00:38,159
And, understandably, BubbleSaur has terrible performance,  and is only ever used to teach how sorting algorithms work.

9
00:00:38,579 --> 00:00:42,299
So algorithms are usually determined by time and space complexity.

10
00:00:42,859 --> 00:00:45,539
No Weinstein, not that time and space.

11
00:00:45,820 --> 00:00:51,100
Meaning, what's the best and worst case scenario of how fast it can go as well as the  amount of space it consumes?

12
00:00:51,600 --> 00:00:58,000
So, the worst case for BubbleSaur is big O of N squared,  while the best case is big O of N.

13
00:00:58,579 --> 00:00:59,600
Here's what that means in English.

14
00:01:00,020 --> 00:01:04,099
N is universally understood as the number of items in the list you want to sort.

15
00:01:04,340 --> 00:01:04,480
List.

16
00:01:05,079 --> 00:01:05,400
Array.

17
00:01:05,640 --> 00:01:06,879
Sorry, my Python is showing.

18
00:01:07,180 --> 00:01:12,659
In our best case scenario, the array is already sorted,  so we need only one loop through them all to determine that.

19
00:01:12,879 --> 00:01:19,359
But on the worst case scenario,  we would have to exponentially do an operation through this array in order for it to be sorted.

20
00:01:19,680 --> 00:01:26,260
Meaning that if there were five items needing to be sorted,  we would have to do over 25 operations in order for it to be sorted.

21
00:01:26,700 --> 00:01:29,079
So, do yourself a favor and just skip this one.

22
00:01:32,460 --> 00:01:37,700
SelectionSaur is known as an in-place comparison sorting algorithm,  and it's actually fairly easy.

23
00:01:38,019 --> 00:01:45,840
This algorithm divides the unsorted array into a sorted sublist,  which starts empty and the remaining items to sort, which is all of it for now.

24
00:01:46,099 --> 00:01:50,879
The algorithm then goes one by one over the unsorted array looking for the smallest number.

25
00:01:50,879 --> 00:01:57,319
Once the array has been completely looked over this iteration,  it then puts the smallest number at the end of the sublist.

26
00:01:57,739 --> 00:02:01,500
This happens until all elements are gone and the array is fully sorted.

27
00:02:01,900 --> 00:02:09,680
So, SelectionSaur has a worst case of Big O and Squared,  as well as the best case scenario of Big O and Squared.  Consistency.

28
00:02:09,919 --> 00:02:13,460
Meaning just like BubbleSaur, SelectionSaur is not ideal at all.

29
00:02:13,719 --> 00:02:17,280
And by the looks of it, arguably worse, right?

30
00:02:17,280 --> 00:02:24,080
Well, SelectionSaur always seems to outperform BubbleSaur in most scenarios,  but there's better choices, so let's move forward.

31
00:02:26,900 --> 00:02:30,139
So, InSertianSaur is very similar to SelectionSaur.

32
00:02:30,259 --> 00:02:33,460
This sorting algorithm does one at a time by constantly comparing.

33
00:02:34,000 --> 00:02:34,979
Here's how it works.

34
00:02:35,360 --> 00:02:40,120
InSertianSaur, like SelectionSaur, it creates two arrays,  one sorted and one unsorted.

35
00:02:40,479 --> 00:02:43,539
It goes through each element one by one in the unsorted array.

36
00:02:43,539 --> 00:02:52,060
With the element that is on, it determines where in the sorted list,  the element belongs to by iterating over each element  to see if the number is between them.

37
00:02:52,340 --> 00:02:57,219
Now, the worst case scenario is Big O and Squared,  while the best case is Big O and.

38
00:02:57,500 --> 00:03:02,780
The thing with InSertianSaur is that it's been implemented  in C++ with only three lines of code.

39
00:03:03,020 --> 00:03:08,020
And what's funny is that this is the sorting algorithm that you use  when you're holding up a bunch of cards and you need to sort them.

40
00:03:08,199 --> 00:03:13,439
So, for very small lists, this might be a great option,  but when you're looking big, no.

41
00:03:16,460 --> 00:03:21,180
MerSaur is a really popular and a really effective comparison-based sorting algorithm.

42
00:03:21,580 --> 00:03:26,560
MerSaur fits in a category you'll see a lot in this video  under the divide and conquer algorithms.

43
00:03:26,939 --> 00:03:31,939
First, take the unsorted array and divide it into the smallest unit,  which is usually just one element.

44
00:03:32,259 --> 00:03:36,500
Then, you compare each element with the array next to it  and sort and merge the list.

45
00:03:36,780 --> 00:03:39,960
Continue until you have one array that is completely sorted.

46
00:03:39,960 --> 00:03:43,819
MerSaur has a worst case performance of Big O and Log N.

47
00:03:44,620 --> 00:03:45,639
Here's what that means.

48
00:03:46,039 --> 00:03:48,360
And, of course, represents the amount of numbers in our list.

49
00:03:48,719 --> 00:03:51,979
Log N represents the amount of times N is divided.

50
00:03:52,439 --> 00:03:56,699
So, if our unsorted array is 16, then Log N would be four.

51
00:03:57,000 --> 00:04:05,620
Since we can divide 16 down to one and four iterations,  and best case is Omega and Log N.  Meaning, it doesn't get faster than this.

52
00:04:05,939 --> 00:04:14,280
Essentially, whether your list is a bit sorted already or completely random,  MerSaur takes about the same amount of time to do his job,  making it reliably efficient.

53
00:04:14,580 --> 00:04:20,139
But, where these divide and conquer algorithms really excel  is the ability to do things in parallel.

54
00:04:20,540 --> 00:04:27,740
Since things are being split into many, many, many sublists,  dividing, sorting, and then merging can exponentially make things faster.

55
00:04:28,079 --> 00:04:29,720
It's actually a really awesome algorithm.

56
00:04:32,720 --> 00:04:39,100
QuickSaur is also one of the most popular algorithms that also uses  the divide and conquer sorting strategy.

57
00:04:39,100 --> 00:04:44,139
Programming languages like JavaScript, Ruby, and PHP use QuickSaur  in their standard library features.

58
00:04:44,420 --> 00:04:46,639
Is that a flex or...

59
00:04:46,639 --> 00:04:50,319
So, QuickSaur works similar to MerSaur,  but requires something called a pivot.

60
00:04:50,660 --> 00:04:53,379
In the unsorted array, pick a number to be your pivot.

61
00:04:53,920 --> 00:04:56,740
This could be the first number, the last number, or the middle number.

62
00:04:57,120 --> 00:05:06,480
The number that is chosen as the pivot is then used to compare  all values into one of each list,  less than or equal to the pivot, or more than the pivot.

63
00:05:06,480 --> 00:05:13,139
A pivot point is then chosen within the newly created list  and do another comparison if it's greater or smaller.

64
00:05:13,480 --> 00:05:16,939
Once it does this recursively to all sub arrays,  it is that considered sorted.

65
00:05:17,879 --> 00:05:23,259
So, the worst case is big O and square,  while the best case is big O and log N.

66
00:05:23,600 --> 00:05:29,660
But, as you can probably guess,  getting that pivot is an extremely important factor  in the performance of this algorithm.

67
00:05:30,040 --> 00:05:32,759
So, some do the first, middle, or last element.

68
00:05:33,000 --> 00:05:38,959
Others just do it randomly,  and then others choose the medium of three small samples  in order to get a general idea.

69
00:05:39,360 --> 00:05:41,259
Otherwise, QuickSaur is coded.

70
00:05:44,400 --> 00:05:50,699
Heapsaur is another comparison-based algorithm,  also known as selectionSaur using the right data structure.

71
00:05:51,160 --> 00:05:52,240
So, yeah.

72
00:05:52,500 --> 00:05:55,240
But first, let's talk about the heap data structure in computer science.

73
00:05:55,639 --> 00:06:03,660
Heap is a tree-based data structure  where all levels are filled except the lowest  and is filled from the left to right,  often used in priority cues.

74
00:06:03,660 --> 00:06:10,000
Max heap is the same concept,  but where the highest numbers start at the top  and lower numbers are pushed down.

75
00:06:10,379 --> 00:06:15,800
And this data structure is used all the time  to give fast operations  as well as a great priority management.

76
00:06:16,139 --> 00:06:18,199
Okay, so here's how heap sort works.

77
00:06:18,439 --> 00:06:23,759
Heapsaur will take an unsorted array  and then start building a max heap  in a process called heapify.

78
00:06:24,019 --> 00:06:25,579
I know, very creative.

79
00:06:25,879 --> 00:06:31,439
This process will constantly validate  if the tree is following proper guidelines  for how a max heap works.

80
00:06:31,439 --> 00:06:35,540
Once the tree is built,  the top element switches with the element at the end.

81
00:06:35,779 --> 00:06:38,300
The heap is then rebuilt and the numbers switch again.

82
00:06:38,600 --> 00:06:40,060
Rinse and repeat until sorted.

83
00:06:40,420 --> 00:06:49,439
And this means that the worst and best case scenario  is a big O and log N.  Almost exactly like insertionSaur,  but doesn't do linear scan of unsorted numbers.

84
00:06:49,800 --> 00:06:51,920
Instead, using the data structure, it provides.

85
00:06:52,540 --> 00:06:54,360
Okay, that was a lot of explaining.

86
00:06:57,300 --> 00:07:00,959
Okay, countingSaur is kind of hilarious,  so here's how it works.

87
00:07:00,959 --> 00:07:02,959
CountingSaur consists of three arrays.

88
00:07:03,480 --> 00:07:07,959
The unsorted array, the counting array,  also known as a collection, and the output array.

89
00:07:08,379 --> 00:07:10,600
First, we define the maximum value in the array.

90
00:07:10,920 --> 00:07:12,740
This defines the size of the counting array.

91
00:07:13,079 --> 00:07:18,180
You iterate over the unsorted array  and for each element,  you add to the count in the counting array.

92
00:07:18,480 --> 00:07:22,100
We then calculate the sum of each number  to the number to the right sequentially.

93
00:07:22,360 --> 00:07:24,519
These numbers represent the amount of numbers before it.

94
00:07:24,699 --> 00:07:30,899
Then we go in reverse order in the input array,  where we insert in the index that is specified  by the value of the number.

95
00:07:31,120 --> 00:07:32,160
Then, subtract by one.

96
00:07:32,379 --> 00:07:34,259
Repeat until everything is sorted.

97
00:07:34,439 --> 00:07:41,500
So the worst-case scenario is big O and plus K,  where K is the range of the smallest number  and the largest number.

98
00:07:41,879 --> 00:07:44,620
It's a funny one,  but sadly, it only works on integer values.

99
00:07:45,019 --> 00:07:48,279
You can't use it to sort colors or names, whatever.

100
00:07:51,519 --> 00:07:56,939
ShellSaur is an in-place comparison algorithm  that is a lot like insertionSaur,  but on steroids.

101
00:07:57,420 --> 00:07:59,079
It works by using intervals.

102
00:07:59,079 --> 00:08:02,079
This starts with the length of the array divided by two.

103
00:08:02,439 --> 00:08:05,639
We then compare the first of each array within the given interval.

104
00:08:05,939 --> 00:08:08,959
If the first number is larger than the second, it swaps.

105
00:08:09,459 --> 00:08:11,379
This array then looks like this.

106
00:08:11,660 --> 00:08:15,699
We then divide our interval in half again  with four elements per sublist.

107
00:08:16,220 --> 00:08:18,160
We iterate over and sort each.

108
00:08:18,439 --> 00:08:22,779
When interval hits one,  we use insertionSaur in order to sort the whole array.

109
00:08:23,139 --> 00:08:29,120
So the best scenario is big O and log N,  while the worst-case scenario is big O and squared.

110
00:08:29,540 --> 00:08:38,200
And the reason why this goes so fast  despite using insertionSaur is  because it creates a partially sorted array  before it does that final sort.

111
00:08:41,180 --> 00:08:45,860
TimSaur was made in 2002  and is still being used to this day in Python.

112
00:08:46,620 --> 00:08:49,279
Well, except for 3.11 plus.

113
00:08:49,960 --> 00:08:53,500
TimSaur separates an array into small sub arrays called runs.

114
00:08:53,500 --> 00:08:56,279
These runs are then sorted using insertionSaur.

115
00:08:56,620 --> 00:09:01,080
Once all the runs have been sorted,  it then starts to use mergeSaur on the two smallest arrays.

116
00:09:01,620 --> 00:09:05,679
After those have been sorted,  it takes the next run and source it into the merged array.

117
00:09:06,139 --> 00:09:08,519
Rinse and repeat until everything's been sorted.

118
00:09:08,879 --> 00:09:20,500
So TimSaur has an average performance of big O and log N.  And TimSaur works really fast  because it gives insertionSaur and mergeSaur a partially sorted  or a smaller array to work with.

119
00:09:20,779 --> 00:09:23,820
Also, the inventor of TimSaur  named it after himself.

120
00:09:24,320 --> 00:09:24,960
Tim Peters.

121
00:09:25,279 --> 00:09:26,740
That's like the biggest flex of all time.

122
00:09:26,899 --> 00:09:27,200
Come on.

123
00:09:30,159 --> 00:09:34,419
So RaticSaur works a lot like countingSaur,  except it's a little bit weird.

124
00:09:35,099 --> 00:09:39,600
RaticSaur starts looking at the last digit in the array  and then source it by that number.

125
00:09:40,000 --> 00:09:44,899
It then looks at the number before the last analyzed number  and then uses that to sort the array again.

126
00:09:45,279 --> 00:09:48,980
And if during an iteration a number is not present,  it just treats it like a zero.

127
00:09:49,399 --> 00:09:53,039
It keeps doing this over and over and over again  until the array is fully sorted.

128
00:09:53,460 --> 00:10:06,559
And the time complexity for RaticSaur is a big O,  sorry, one second, D times N plus B.  D is the amount of numbers in the largest number  and is the number of elements  and B is the base of the numbers being used.

129
00:10:06,860 --> 00:10:09,080
So 10 will represent zero to nine.

130
00:10:09,340 --> 00:10:15,580
And just like a lot of the algorithms  we've already talked about,  RaticSaur can be used in parallel to go even faster.

131
00:10:15,960 --> 00:10:20,059
And fun fact, RaticSaur is old  and I mean really old.

132
00:10:20,059 --> 00:10:22,580
Like around 1887 old.

133
00:10:23,000 --> 00:10:27,840
It was also used in the old IBM punch card machines  that go all the way back to the 1920s.

134
00:10:28,039 --> 00:10:31,039
Let me know in the comments if you want me to go a little bit more  in depth on these algorithms.

135
00:10:31,500 --> 00:10:33,960
Let me know what concept you want me to easily explain on this channel.

136
00:10:34,159 --> 00:10:42,279
If you're interested, I have a video  where I give 40 APIs you should use for your next project  as well as a video where I give $10,000  to the worst trading bot I've ever built.

